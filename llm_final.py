# -*- coding: utf-8 -*-
"""llm_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jWFWLMEisVq-TeF-smhCgtwUa0Fk2zqD

#Load LLM
"""

from google.colab import drive
import os

# --- Mount Google Drive ---
drive.mount('/content/drive')

!pip install -U transformers

!pip install -U bitsandbytes
!pip install accelerate peft

"""#Qwen/Qwen3-4B-Instruct-2507"""

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model_id = "Qwen/Qwen3-4B-Instruct-2507"

# B∆∞·ªõc 1: c·∫•u h√¨nh 4-bit cho QLoRA
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="bfloat16"
)

# B∆∞·ªõc 2: load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

# B∆∞·ªõc 3: chu·∫©n b·ªã QLoRA
model = prepare_model_for_kbit_training(model)

# B∆∞·ªõc 4: c·∫•u h√¨nh LoRA
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # th∆∞·ªùng √°p d·ª•ng cho LLaMA/Qwen
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# Test inference nh∆∞ c≈©
messages = [
    {"role": "user", "content": "Who are you?"},
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

messages = [
    {"role": "user", "content": "Th·ªß ƒë√¥ c·ªßa vi·ªát nam l√† g√¨?"}
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

def chat(model, tokenizer, messages, max_new_tokens=100):
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

messages = [{"role": "user", "content": "What is the capital of France?"}]
print(chat(model, tokenizer, messages))

messages.append({"role": "user", "content": "And the capital of Germany?"})
print(chat(model, tokenizer, messages))

"""#microsoft/Phi-3-mini-4k-instruct"""

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

model_id = "microsoft/Phi-3-mini-4k-instruct"

# B∆∞·ªõc 1: c·∫•u h√¨nh 4-bit cho QLoRA (GI·ªÆ NGUY√äN)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# B∆∞·ªõc 2: load tokenizer & model (CH·ªà TH√äM trust_remote_code + pad_token)
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    use_cache=False,          # ‚¨ÖÔ∏è R·∫§T QUAN TR·ªåNG
)


# B∆∞·ªõc 3: chu·∫©n b·ªã QLoRA (GI·ªÆ NGUY√äN)
model = prepare_model_for_kbit_training(model)

# B∆∞·ªõc 4: c·∫•u h√¨nh LoRA (GI·ªÆ NGUY√äN Y H·ªÜT Qwen)
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["qkv_proj"],  # ‚¨ÖÔ∏è B·∫ÆT BU·ªòC
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)


model = get_peft_model(model, lora_config)

# Test inference nh∆∞ c≈© (GI·ªÆ NGUY√äN)
messages = [
    {"role": "user", "content": "Who are you?"}
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

model.config.use_cache = False
model.generation_config.use_cache = False   # ‚¨ÖÔ∏è D√íNG QUY·∫æT ƒê·ªäNH


outputs = model.generate(
    **inputs,
    max_new_tokens=40,
    use_cache=False          # ‚¨ÖÔ∏è √âP L·∫¶N CU·ªêI
)



print(
    tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[-1]:],
        skip_special_tokens=True
    )
)

def chat_phi3(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens=50,
    temperature=0.2,
    top_p=0.9
):
    """
    Chat ƒë∆°n gi·∫£n cho Phi-3.
    T·∫ÆT cache ho√†n to√†n ƒë·ªÉ tr√°nh l·ªói DynamicCache.
    """

    # Phi-3 d√πng system + user
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]

    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # T·∫Øt cache TRI·ªÜT ƒê·ªÇ
    model.config.use_cache = False
    model.generation_config.use_cache = False

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,          # ABSA c·∫ßn ·ªïn ƒë·ªãnh
            temperature=temperature,
            top_p=top_p,
            use_cache=False           # ‚¨ÖÔ∏è C·ª∞C K·ª≤ QUAN TR·ªåNG
        )

    return tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[-1]:],
        skip_special_tokens=True
    )

"""#Split original sentences + Extract TERM"""

def split_sentence_with_terms_llm(sentence, model, tokenizer, max_new_tokens=300):
    """
    T√°ch m·ªôt sentence th√†nh c√°c clause + extract term/aspect ch·ªâ b·∫±ng LLM.
    Tr·∫£ v·ªÅ list dict:
    [{"clause": ..., "term": ..., "sentence_original": ...}, ...]
    """
    prompt = (
    "You are an expert linguist working on Aspect-Based Sentiment Analysis (ABSA).\n"
    "Your task is to split the following review sentence into smaller clauses and identify the aspect/term discussed in each clause.\n\n"

    "==================== STRICT RULES ====================\n"
    "1Ô∏è. DO NOT add, remove, translate, explain, or modify ANY words, symbols, or punctuation in the original sentence.\n"
    "   ‚Ä¢ Every clause must be a **continuous substring** of the original sentence.\n"
    "   ‚Ä¢ The output must cover **all parts of the sentence** ‚Äî no content should be ignored or missing.\n"
    "2Ô∏è. Only split the sentence where it makes sense semantically ‚Äî typically around conjunctions ('and', 'but', 'while', 'although', etc.) "
    "or when the opinion changes.\n"
    "   ‚Ä¢Do NOT split phrases that grammatically or logically belong to the same subject. "
    "   ‚Ä¢ If a descriptive phrase does not have a clear term in the sentence, keep it as a separate clause but leave Term blank."
    "3Ô∏è. Keep the exact original wording and order in each clause. Do NOT reorder, paraphrase, or summarize.\n"
    "4Ô∏è. Each clause must express a clear **opinion or evaluative meaning**, either explicit (e.g., 'dirty', 'perfect') or implicit "
    "(e.g., 'gave us many tips' implies helpfulness, 'helped us with departure' implies good service).\n"
    "5Ô∏è. Do NOT separate adverbs (e.g., 'really', 'very', 'so', 'too', 'quite', 'extremely', 'absolutely', "
    "'rather', 'fairly', 'pretty', 'incredibly', 'particularly', 'deeply', 'highly') from the words they modify.\n"
    "6Ô∏è. Keep negative or limiting words such as 'nothing', 'none', 'nobody', 'no one', 'nowhere', 'never', "
    "'hardly', 'barely', 'scarcely', 'without', 'no', 'not' **inside the same clause** ‚Äî they must not be removed or separated.\n"
    "7Ô∏è. Identify the **TERM** being discussed in each clause.\n"
    "   ‚Ä¢ TERM: the main aspect or entity being described (e.g., 'staff', 'room', 'hotel').\n"
    "   ‚Ä¢ If no clear term appears, leave it blank.\n"
    "8Ô∏è. Avoid creating meaningless or redundant clauses.\n"
    "9Ô∏è. If multiple terms appear in the same clause, separate them with commas.\n"
    "10Ô∏è. If a clause refers to the same entity as a previous one but does not repeat it explicitly, "
    "**propagate the term from the previous clause**.\n\n"

    "==================== COVERAGE REQUIREMENT ====================\n"
    " Every part of the original sentence must appear in at least one clause.\n"
    " Do NOT skip, shorten, or drop any meaningful phrase, even if it lacks an explicit sentiment word.\n"
    " Clauses that describe actions, experiences, or behaviors with clear positive/negative implications "
    "must be included (e.g., 'gave us many tips', 'helped us with departure').\n\n"

    "==================== OUTPUT FORMAT ====================\n"
    "Clause: <clause text> | Term: <term1,term2,...>\n\n"

    "==================== EXAMPLES ====================\n"
    "Input: The apartment was fully furnished, great facilities, everything was cleaned and well prepared.\n"
    "Output:\n"
    "Clause: The apartment was fully furnished | Term: apartment\n"
    "Clause: great facilities | Term: facilities\n"
    "Clause: everything was cleaned and well prepared | Term: room,facility\n\n"

    "Input: diny was really helpful, he gave us many tips and helped us with departure.\n"
    "Output:\n"
    "Clause: diny was really helpful | Term: staff\n"
    "Clause: he gave us many tips | Term: staff\n"
    "Clause: helped us with departure | Term: staff\n\n"

    "Input: i can definitely recommend it!.\n"
    "Output:\n"
    "Clause: i can definitely recommend it! | Term: \n\n"

    "==================== RESPONSE INSTRUCTION ====================\n"
    "Respond ONLY with the clauses and terms exactly in the format shown above.\n"
    "Do NOT include any explanation, reasoning, or commentary.\n"
    "Do NOT include quotation marks, markdown, or extra text.\n\n"

    f"Now process this sentence WITHOUT changing any words:\n{sentence}"
)


    messages = [{"role": "user", "content": prompt}]
    response = chat(model, tokenizer, messages, max_new_tokens=max_new_tokens).strip()

    # --- L√†m s·∫°ch output ---
    result = []
    last_term = ""
    for line in response.split("\n"):
        line = line.strip()
        if not line:
            continue
        if "| Term:" in line:
            clause_text, term = line.split("| Term:")
            clause_text = clause_text.replace("Clause:", "").strip()
            term = term.strip()
            if term == "":
                term = last_term  # propagate term
            else:
                last_term = term
        else:
            clause_text = line
            term = last_term
        result.append({"clause": clause_text, "term": term, "sentence_original": sentence})

    return result

sentence = "The room was clean but the staff were not very helpful and the location was perfect."

result = split_sentence_with_terms_llm(sentence, model, tokenizer)

for r in result:
    print(r)

def split_sentence_with_terms_llm(sentence, model, tokenizer, max_new_tokens=300):
    """
    T√°ch m·ªôt sentence th√†nh c√°c clause + extract term/aspect ch·ªâ b·∫±ng LLM.
    Tr·∫£ v·ªÅ list dict:
    [{"clause": ..., "term": ..., "sentence_original": ...}, ...]
    """
    prompt = (
    "You are an expert linguist working on Aspect-Based Sentiment Analysis (ABSA).\n"
    "Your task is to split the following review sentence into smaller clauses and identify the aspect/term discussed in each clause.\n\n"

    "==================== STRICT RULES ====================\n"
    "1Ô∏è. DO NOT add, remove, translate, explain, or modify ANY words, symbols, or punctuation in the original sentence.\n"
    "   ‚Ä¢ Every clause must be a **continuous substring** of the original sentence.\n"
    "   ‚Ä¢ The output must cover **all parts of the sentence** ‚Äî no content should be ignored or missing.\n"
    "2Ô∏è. Only split the sentence where it makes sense semantically ‚Äî typically around conjunctions ('and', 'but', 'while', 'although', etc.) "
    "or when the opinion changes.\n"
    "   ‚Ä¢Do NOT split phrases that grammatically or logically belong to the same subject. "
    "   ‚Ä¢ If a descriptive phrase does not have a clear term in the sentence, keep it as a separate clause but leave Term blank."
    "3Ô∏è. Keep the exact original wording and order in each clause. Do NOT reorder, paraphrase, or summarize.\n"
    "4Ô∏è. Each clause must express a clear **opinion or evaluative meaning**, either explicit (e.g., 'dirty', 'perfect') or implicit "
    "(e.g., 'gave us many tips' implies helpfulness, 'helped us with departure' implies good service).\n"
    "5Ô∏è. Do NOT separate adverbs (e.g., 'really', 'very', 'so', 'too', 'quite', 'extremely', 'absolutely', "
    "'rather', 'fairly', 'pretty', 'incredibly', 'particularly', 'deeply', 'highly') from the words they modify.\n"
    "6Ô∏è. Keep negative or limiting words such as 'nothing', 'none', 'nobody', 'no one', 'nowhere', 'never', "
    "'hardly', 'barely', 'scarcely', 'without', 'no', 'not' **inside the same clause** ‚Äî they must not be removed or separated.\n"
    "7Ô∏è. Identify the **TERM** being discussed in each clause.\n"
    "   ‚Ä¢ TERM: the main aspect or entity being described (e.g., 'staff', 'room', 'hotel').\n"
    "   ‚Ä¢ If no clear term appears, leave it blank.\n"
    "8Ô∏è. Avoid creating meaningless or redundant clauses.\n"
    "9Ô∏è. If multiple terms appear in the same clause, separate them with commas.\n"
    "10Ô∏è. If a clause refers to the same entity as a previous one but does not repeat it explicitly, "
    "**propagate the term from the previous clause**.\n\n"

    "==================== COVERAGE REQUIREMENT ====================\n"
    " Every part of the original sentence must appear in at least one clause.\n"
    " Do NOT skip, shorten, or drop any meaningful phrase, even if it lacks an explicit sentiment word.\n"
    " Clauses that describe actions, experiences, or behaviors with clear positive/negative implications "
    "must be included (e.g., 'gave us many tips', 'helped us with departure').\n\n"

    "==================== OUTPUT FORMAT ====================\n"
    "Clause: <clause text> | Term: <term1,term2,...>\n\n"

    "==================== EXAMPLES ====================\n"
    "Input: The apartment was fully furnished, great facilities, everything was cleaned and well prepared.\n"
    "Output:\n"
    "Clause: The apartment was fully furnished | Term: apartment\n"
    "Clause: great facilities | Term: facilities\n"
    "Clause: everything was cleaned and well prepared | Term: room,facility\n\n"

    "Input: diny was really helpful, he gave us many tips and helped us with departure.\n"
    "Output:\n"
    "Clause: diny was really helpful | Term: staff\n"
    "Clause: he gave us many tips | Term: staff\n"
    "Clause: helped us with departure | Term: staff\n\n"

    "Input: i can definitely recommend it!.\n"
    "Output:\n"
    "Clause: i can definitely recommend it! | Term: \n\n"

    "==================== RESPONSE INSTRUCTION ====================\n"
    "Respond ONLY with the clauses and terms exactly in the format shown above.\n"
    "Do NOT include any explanation, reasoning, or commentary.\n"
    "Do NOT include quotation marks, markdown, or extra text.\n\n"

    f"Now process this sentence WITHOUT changing any words:\n{sentence}"
)


    # ‚¨ÖÔ∏è D√ôNG CHAT PHI-3
    response = chat_phi3(
        model,
        tokenizer,
        prompt,
        max_new_tokens=max_new_tokens
    ).strip()

    # ===== Parse output =====
    result = []
    last_term = ""

    for line in response.split("\n"):
        line = line.strip()
        if not line:
            continue

        if "| Term:" in line:
            clause_text, term = line.split("| Term:")
            clause_text = clause_text.replace("Clause:", "").strip()
            term = term.strip()

            if term == "":
                term = last_term
            else:
                last_term = term
        else:
            clause_text = line
            term = last_term

        result.append({
            "clause": clause_text,
            "term": term,
            "sentence_original": sentence
        })

    return result

"""##Term Refinement & Normalization"""

def split_and_term_extraction(sentence):
  clauses = split_sentence_with_terms_llm(sentence, model=model, tokenizer=tokenizer)
  if clauses:
    last_clause = clauses[-1]
    if "term" in last_clause:
        # Lo·∫°i b·ªè chu·ªói '<|im_end|>' n·∫øu xu·∫•t hi·ªán
        last_clause["term"] = last_clause["term"].replace("<|im_end|>", "").strip()
  for c in clauses:
    terms = [t.strip() for t in c.get("term", "").split(",") if t.strip()]
    # Ch·ªâ gi·ªØ term xu·∫•t hi·ªán trong sentence_original
    terms = [t for t in terms if t.lower() in c["sentence_original"].lower()]
    c["term"] = ",".join(terms)
  return clauses

sentence = "wifi is good"

clauses=split_and_term_extraction(sentence)

clauses

"""#Extract OPINION

##Qwen
"""

import re

def extract_opinions_only_from_clauses(clauses, model, tokenizer, max_new_tokens=40):
    """
    Extract ONLY opinion expressions for each clause in hotel domain.
    Full strict rules + same output cleaning logic as original code.
    """

    final_clauses = []

    for c in clauses:
        clause_text = c["sentence_original"]
        term = c.get("term", "")
        sentence_original = c.get("sentence_original", "")

        prompt = (
        "You are an expert linguist working on Aspect-Based Sentiment Analysis (ABSA) "
        "in the domain of hotel and hospitality reviews.\n\n"

        "==================== DOMAIN KNOWLEDGE ====================\n"
        "Common opinion expressions:\n"
        "‚Ä¢ Cleanliness: clean, dirty, spotless, dusty\n"
        "‚Ä¢ Service attitude: friendly, rude, helpful, unprofessional\n"
        "‚Ä¢ Comfort: comfortable, noisy, spacious, small\n"
        "‚Ä¢ Food: delicious, cold, amazing, awful\n"
        "‚Ä¢ Value: expensive, overpriced, worth it\n"
        "‚Ä¢ Location: convenient, far away, perfect location\n"
        "‚Ä¢ Overall: perfect, terrible, disappointing, fantastic\n\n"

        "==================== STRICT RULES ====================\n"
        "1Ô∏è‚É£ Must keep original wording only.\n"
        "2Ô∏è‚É£ Opinion words must appear exactly in the clause.\n"
        "3Ô∏è‚É£ Only extract evaluative expressions.\n"
        "4Ô∏è‚É£ Must describe or evaluate the Term:\n"
        f"     ‚Üí Term: '{term}'\n"
        "5Ô∏è‚É£ Do NOT guess or invent opinions.\n"
        "6Ô∏è‚É£ If no clear opinion: return empty.\n"
        "7Ô∏è‚É£ Respond ONLY in required format.\n\n"

        "==================== OUTPUT FORMAT ====================\n"
        "Opinion: <opinion1, opinion2, ...>\n"
        "(comma separated)\n\n"

        "==================== RESPONSE INSTRUCTION ====================\n"
        "No extra comments. No explanations. Only answer.\n\n"

        f"Clause:\n{clause_text}"
        )

        messages = [{"role": "user", "content": prompt}]
        opinion_text = chat(model, tokenizer, messages,
                            max_new_tokens=max_new_tokens).strip()

        # === CLEAN OUTPUT ===
        opinion_text = (
            opinion_text.replace("<|im_end|>", "")
            .replace("Opinion:", "")
            .replace("\n", " ")
            .strip()
        )

        opinions = [o.strip() for o in re.split(r",", opinion_text) if o.strip()]

        # === VALIDATE: MUST appear in original sentence ===
        valid_opinions = [
            o for o in opinions
            if re.search(rf"\b{re.escape(o)}\b", sentence_original, re.IGNORECASE)
        ]

        new_c = c.copy()
        new_c["opinion"] = ", ".join(valid_opinions) if valid_opinions else ""
        final_clauses.append(new_c)

    return final_clauses

"""##Phi"""

import re

def extract_opinions_only_from_clauses(clauses, model, tokenizer, max_new_tokens=40):
    """
    Extract ONLY opinion expressions for each clause in hotel domain.
    Full strict rules + same output cleaning logic as original code.
    """

    final_clauses = []

    for c in clauses:
        clause_text = c["sentence_original"]
        term = c.get("term", "")
        sentence_original = c.get("sentence_original", "")

        prompt = (
        "You are an expert linguist working on Aspect-Based Sentiment Analysis (ABSA) "
        "in the domain of hotel and hospitality reviews.\n\n"

        "==================== DOMAIN KNOWLEDGE ====================\n"
        "Common opinion expressions:\n"
        "‚Ä¢ Cleanliness: clean, dirty, spotless, dusty\n"
        "‚Ä¢ Service attitude: friendly, rude, helpful, unprofessional\n"
        "‚Ä¢ Comfort: comfortable, noisy, spacious, small\n"
        "‚Ä¢ Food: delicious, cold, amazing, awful\n"
        "‚Ä¢ Value: expensive, overpriced, worth it\n"
        "‚Ä¢ Location: convenient, far away, perfect location\n"
        "‚Ä¢ Overall: perfect, terrible, disappointing, fantastic\n\n"

        "==================== STRICT RULES ====================\n"
        "1Ô∏è‚É£ Must keep original wording only.\n"
        "2Ô∏è‚É£ Opinion words must appear exactly in the clause.\n"
        "3Ô∏è‚É£ Only extract evaluative expressions.\n"
        "4Ô∏è‚É£ Must describe or evaluate the Term:\n"
        f"     ‚Üí Term: '{term}'\n"
        "5Ô∏è‚É£ Do NOT guess or invent opinions.\n"
        "6Ô∏è‚É£ If no clear opinion: return empty.\n"
        "7Ô∏è‚É£ Respond ONLY in required format.\n\n"

        "==================== OUTPUT FORMAT ====================\n"
        "Opinion: <opinion1, opinion2, ...>\n"
        "(comma separated)\n\n"

        "==================== RESPONSE INSTRUCTION ====================\n"
        "No extra comments. No explanations. Only answer.\n\n"

        f"Clause:\n{clause_text}"
        )

        # ‚¨ÖÔ∏è G·ªåI PHI-3 CHAT
        opinion_text = chat_phi3(
            model,
            tokenizer,
            prompt,
            max_new_tokens=max_new_tokens
        ).strip()

        # ===== CLEAN OUTPUT =====
        opinion_text = (
            opinion_text.replace("Opinion:", "")
            .replace("<|im_end|>", "")
            .replace("\n", " ")
            .strip()
        )

        opinions = [
            o.strip() for o in re.split(r",", opinion_text)
            if o.strip()
        ]

        # ===== VALIDATE: must appear in original sentence =====
        valid_opinions = [
            o for o in opinions
            if re.search(
                rf"\b{re.escape(o)}\b",
                sentence_original,
                re.IGNORECASE
            )
        ]

        new_c = c.copy()
        new_c["opinion"] = ", ".join(valid_opinions) if valid_opinions else ""
        final_clauses.append(new_c)

    return final_clauses

clauses = extract_opinions_only_from_clauses(clauses, model, tokenizer)

clauses



"""#Finetune Roberta base

##L·ªçc c√°c c√¢u kh√¥ng ph·∫£i ti·∫øng anh
"""

!pip install langdetect

from langdetect import detect, DetectorFactory

DetectorFactory.seed = 0  # ƒë·ªÉ langdetect ·ªïn ƒë·ªãnh

def translate_to_english(sentence, model, tokenizer, max_new_tokens=200):
    """
    D·ªãch m·ªôt c√¢u sang ti·∫øng Anh n·∫øu n√≥ KH√îNG ph·∫£i ti·∫øng Anh.
    - N·∫øu l√† ti·∫øng Anh r·ªìi th√¨ gi·ªØ nguy√™n.
    - N·∫øu kh√¥ng ph·∫£i, d·ªãch sang ti·∫øng Anh b·∫±ng LLM.
    - N·∫øu l·ªói ph√°t hi·ªán ng√¥n ng·ªØ, v·∫´n th·ª≠ d·ªãch.

    Tr·∫£ v·ªÅ: c√¢u ti·∫øng Anh (string)
    """
    if not isinstance(sentence, str) or sentence.strip() == "":
        return ""

    text = sentence.strip()

    # H√†m ph·ª• ki·ªÉm tra ti·∫øng Anh
    def is_english(text):
        try:
            return detect(text) == "en"
        except:
            return False

    # N·∫øu kh√¥ng ph·∫£i ti·∫øng Anh ‚Üí D·ªãch
    if not is_english(text):
        prompt = f"""
You are a professional translator.

Task: Translate the following text into natural English, preserving its meaning.
Do not explain, comment, or add anything else.

Text:
{text}

Translation:
"""
        messages = [{"role": "user", "content": prompt}]
        translated = chat(model, tokenizer, messages, max_new_tokens=max_new_tokens).strip()
        return translated

    # N·∫øu l√† ti·∫øng Anh ‚Üí gi·ªØ nguy√™n
    return text

import pandas as pd

goal_path = "/content/500 sample.csv"
goal_df = pd.read_csv(goal_path)

goal_df["category"].unique()

# ============================================================
# === FINETUNE CATEGORY CLASSIFIER (LoRA) V·ªöI GOAL DATA =====
# ============================================================

!pip install -q transformers datasets peft accelerate bitsandbytes scikit-learn langdetect

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
from sklearn.model_selection import train_test_split
from langdetect import detect, DetectorFactory

# ====================== C·∫§U H√åNH ======================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Using device: {device}")

# ====================== ƒê·ªåC D·ªÆ LI·ªÜU ======================
# goal_path = "/content/500 sample.csv"
# goal_df = pd.read_csv(goal_path)

DetectorFactory.seed = 0

goal_df["sentence_original"] = goal_df["sentence_original"].astype(str).str.strip()
goal_df["word_count"] = goal_df["sentence_original"].apply(lambda x: len(x.split()))

removed_short = goal_df[goal_df["word_count"] <= 1]
count_removed_short = len(removed_short)

goal_df = goal_df[goal_df["word_count"] > 1]
goal_df = goal_df[goal_df["sentence_original"].apply(lambda x: detect(x) == "en")]

print(f"S·ªë c√¢u c√≥ \u2264 1 t·ª´ b·ªã lo·∫°i: {count_removed_short}")


assert {"clause", "category"}.issubset(goal_df.columns), "goal.csv thi·∫øu c·ªôt 'clause' ho·∫∑c 'category'"
print(f" goal_df: {len(goal_df)} m·∫´u")
print(goal_df.head(3))

# ====================== CHIA T·∫¨P TRAIN / TEST ======================
train_df, eval_df = train_test_split(goal_df, test_size=0.1, random_state=42, stratify=goal_df["category"])

# ====================== TOKENIZER + ENCODING ======================
model_name = "roberta-base"
tokenizer_cat = AutoTokenizer.from_pretrained(model_name)

label_list = sorted(goal_df["category"].unique().tolist())
label2id = {l: i for i, l in enumerate(label_list)}
id2label = {i: l for l, i in label2id.items()}

def encode_fn(batch):
    enc = tokenizer_cat(batch["clause"], truncation=True, padding="max_length", max_length=128)
    enc["labels"] = label2id[batch["category"]]
    return enc

train_ds = Dataset.from_pandas(train_df).map(encode_fn)
eval_ds = Dataset.from_pandas(eval_df).map(encode_fn)

train_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
eval_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# ====================== C·∫§U H√åNH LoRA ======================
print("\nKh·ªüi t·∫°o m√¥ h√¨nh v√† c·∫•u h√¨nh LoRA ...")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_CLS
)

model_cat = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label_list),
    ignore_mismatched_sizes=True
)
model_cat = get_peft_model(model_cat, lora_config)
model_cat.to(device)
model_cat.print_trainable_parameters()

# ====================== HU·∫§N LUY·ªÜN ======================
args = TrainingArguments(
    output_dir="./roberta_lora_goal",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    num_train_epochs=50,
    learning_rate=1.5e-4,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    weight_decay=0.05,
    save_strategy="epoch",
    logging_steps=10,
    fp16=torch.cuda.is_available(),
)

trainer = Trainer(
    model=model_cat,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    tokenizer=tokenizer_cat,
)

print("\n B·∫Øt ƒë·∫ßu fine-tuning v·ªõi GOAL dataset ...")
trainer.train()

# ====================== L∆ØU M√î H√åNH SAU HU·∫§N LUY·ªÜN ======================
save_path = "/content/roberta_lora_category_goal"
model_cat.save_pretrained(save_path)
tokenizer_cat.save_pretrained(save_path)

print(f"\n Fine-tuning ho√†n t·∫•t! M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {save_path}")

"""#Extract CATEGORY"""

# ====================== PREDICT CATEGORY CHO CLAUSES ======================
def extract_category(clauses, model,tokenizer):
    model.eval()
    model.to(device)

    for c in clauses:
        # Modified to use 'sentence_original' if 'clause' is not present
        text = str(c.get("clause", c.get("sentence_original", ""))).strip()
        if text == "":
            c["category"] = "Unknown"
            continue

        # CH√ö √ù: d√πng tokenizer_cat
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=128
        ).to(device)

        try:
            with torch.no_grad():
                outputs = model(**inputs)
                pred_id = torch.argmax(outputs.logits, dim=1).item()
                c["category"] = id2label[pred_id]
        except Exception as e:
            # N·∫øu g·∫∑p l·ªói GPU, in ra ƒë·ªÉ debug v√† g√°n Unknown
            print(f" L·ªói khi x·ª≠ l√Ω clause='{text}': {e}")
            c["category"] = "Unknown"

    return clauses

extract_category(clauses,model_cat,tokenizer_cat)

"""#Extract POLARITY"""

from transformers import pipeline
from tqdm import tqdm

from transformers import pipeline
from tqdm import tqdm

# --- Kh·ªüi t·∫°o pipeline ---
polarity_classifier = pipeline(
    "text-classification",
    model="yangheng/deberta-v3-base-absa-v1.1",
    top_k=None,   # l·∫•y t·∫•t c·∫£ logits, sau ƒë√≥ ch·ªçn nh√£n cao nh·∫•t
    truncation=True
)

# --- H√†m ph√°t hi·ªán polarity ---
def detect_polarity(clauses):
    results = []
    for item in clauses:
        clause = str(item.get("clause", "")).strip()
        if clause == "":
            item["polarity"] = "Neutral"
            item["polarity_score"] = 0.0
            results.append(item)
            continue

        try:
            res = polarity_classifier(clause)
            # M·ªôt s·ªë model Hugging Face tr·∫£ v·ªÅ list l·ªìng list -> x·ª≠ l√Ω ƒë·ªÉ l·∫•y nh√£n cao nh·∫•t
            if isinstance(res, list) and isinstance(res[0], list):
                res = res[0]
            top = max(res, key=lambda x: x["score"])
            item["polarity"] = top["label"].capitalize()
            item["polarity_score"] = round(top["score"], 4)
        except Exception as e:
            print(f" L·ªói khi x·ª≠ l√Ω c√¢u '{clause}': {e}")
            item["polarity"] = "Neutral"
            item["polarity_score"] = 0.0

        results.append(item)
    return results

# --- G·ªçi h√†m ---
clauses_with_polarity = detect_polarity(clauses)

# --- In ch·ªâ polarity ---
for c in clauses_with_polarity:
    print(f"{c['clause']} ‚Üí {c['polarity']}")

clauses

clauses

"""#ABSA pipeline"""

###Kh√¥ng SPlit

"""##Qwen"""

import re

def extract_terms_only_from_sentence(sentence, model, tokenizer, max_new_tokens=20):
    """
    Extract only TERMS (aspects) from whole sentence using LLM.
    Do NOT split sentence into clauses anymore.
    """

    prompt = (
        "You are an expert linguist specializing in Aspect-Based Sentiment Analysis (ABSA).\n"
        "Your task: Identify the **aspect/term** that is being described or evaluated in the entire sentence below.\n\n"

        "==================== DOMAIN ====================\n"
        "Domain: HOTEL reviews\n\n"

        "==================== STRICT RULES ====================\n"
        "1. TERM must appear as an explicit entity/aspect in the sentence.\n"
        "2. Do NOT paraphrase, translate, or create new terms.\n"
        "3. Term must be a noun related to hotel domain (e.g., staff, room,rooms, service, location, facility)\n"
        "4. If multiple terms appear ‚Üí separate them by commas.\n"
        "5. If no clear term appears ‚Üí leave it blank.\n\n"

        "==================== OUTPUT FORMAT ====================\n"
        "Term: <term1,term2,...>\n\n"

        "==================== RESPONSE INSTRUCTION ====================\n"
        "Respond ONLY with the term list.\n"
        "Do NOT include: Clause, quotes, explanation, extra text.\n\n"

        f"Sentence:\n{sentence}\n\n"
        "Answer:"
    )

    messages = [{"role": "user", "content": prompt}]
    term_text = chat(model, tokenizer, messages, max_new_tokens=max_new_tokens).strip()

    # L√†m s·∫°ch output
    term_text = (
        term_text.replace("<|im_end|>", "")
        .replace("Term:", "")
        .replace("\n", " ")
        .strip()
    )

    # Chu·∫©n h√≥a danh s√°ch term
    terms = re.split(r",", term_text)
    terms = [t.strip() for t in terms if t.strip()]

    # Lo·∫°i term kh√¥ng c√≥ trong c√¢u ‚Üí tu√¢n th·ªß Rule
    valid_terms = [
        t for t in terms if re.search(rf"\b{re.escape(t)}\b", sentence, re.IGNORECASE)
    ]

    return [{
        "sentence_original": sentence,
        "term": ", ".join(valid_terms) if valid_terms else "",
        "clause": sentence
    }]

"""##Phi"""

import re

def extract_terms_only_from_sentence_phi(sentence, model, tokenizer, max_new_tokens=20):
    """
    Extract only TERMS (aspects) from whole sentence using Phi-3.
    Do NOT split sentence into clauses.
    """

    # ===== SAFETY: ƒë·∫£m b·∫£o sentence l√† string =====
    if isinstance(sentence, list):
        sentence = " ".join([str(s) for s in sentence if s])
    if not isinstance(sentence, str):
        sentence = str(sentence)

    sentence = sentence.strip()

    if not sentence:
        return [{
            "sentence_original": "",
            "term": "",
            "clause": ""
        }]

    prompt = (
        "You are an expert linguist specializing in Aspect-Based Sentiment Analysis (ABSA).\n"
        "Your task: Identify the **aspect/term** that is being described or evaluated in the entire sentence below.\n\n"

        "==================== DOMAIN ====================\n"
        "Domain: HOTEL reviews\n\n"

        "==================== STRICT RULES ====================\n"
        "1. TERM must appear as an explicit entity/aspect in the sentence.\n"
        "2. Do NOT paraphrase, translate, or create new terms.\n"
        "3. Term must be a noun related to hotel domain "
        "(e.g., staff, room, rooms, service, location, facility).\n"
        "4. If multiple terms appear ‚Üí separate them by commas.\n"
        "5. If no clear term appears ‚Üí leave it blank.\n\n"

        "==================== OUTPUT FORMAT ====================\n"
        "Term: <term1,term2,...>\n\n"

        "==================== RESPONSE INSTRUCTION ====================\n"
        "Respond ONLY with the term list.\n"
        "Do NOT include Clause, quotes, explanation, or extra text.\n\n"

        f"Sentence:\n{sentence}"
    )

    # ‚¨ÖÔ∏è D√ôNG CHAT PHI-3 (KH√îNG messages)
    term_text = chat_phi3(
        model,
        tokenizer,
        prompt,
        max_new_tokens=max_new_tokens
    ).strip()

    # ===== CLEAN OUTPUT =====
    term_text = (
        term_text.replace("<|im_end|>", "")
        .replace("Term:", "")
        .replace("\n", " ")
        .strip()
    )

    # ===== SPLIT & NORMALIZE =====
    terms = [t.strip() for t in term_text.split(",") if t.strip()]

    # ===== VALIDATE: term ph·∫£i xu·∫•t hi·ªán trong sentence =====
    valid_terms = [
        t for t in terms
        if re.search(rf"\b{re.escape(t)}\b", sentence, re.IGNORECASE)
    ]

    return [{
        "sentence_original": sentence,
        "term": ", ".join(valid_terms) if valid_terms else "",
        "clause": sentence
    }]

import pandas as pd
from tqdm import tqdm
import torch
from langdetect import detect, DetectorFactory
import os

DetectorFactory.seed = 0  # ƒë·ªÉ langdetect ·ªïn ƒë·ªãnh


# ==============================
# H√†m d·ªãch sang ti·∫øng Anh n·∫øu c·∫ßn
# ==============================
def translate_to_english(sentence, model, tokenizer, max_new_tokens=200):
    """
    D·ªãch sang ti·∫øng Anh n·∫øu c√¢u KH√îNG ph·∫£i ti·∫øng Anh.
    Gi·ªØ nguy√™n n·∫øu ƒë√£ l√† ti·∫øng Anh.
    """
    if not isinstance(sentence, str) or sentence.strip() == "":
        return ""

    text = sentence.strip()

    # --- ki·ªÉm tra ng√¥n ng·ªØ ---
    try:
        is_en = detect(text) == "en"
    except:
        is_en = False

    if not is_en:
        prompt = f"""
You are a professional translator.

Task: Translate the following text into natural English.
Do not explain, comment, or add anything else.

Text:
{text}

Translation:
"""
        messages = [{"role": "user", "content": prompt}]
        translated = chat(model, tokenizer, messages, max_new_tokens=max_new_tokens).strip()
        return translated

    return text


# ==============================
# Pipeline ch√≠nh
# ==============================
def absa_pipeline(
    sentences,
    model_qwen,
    tokenizer_qwen,
    model_cat,
    tokenizer_cat,
    id2label,
    device,
    save_every=100,
    save_path="/content/drive/MyDrive/ABSA_results/absa_results",
):
    """
    ABSA Pipeline c√≥ th√™m b∆∞·ªõc d·ªãch (ghi ƒë√® c√¢u g·ªëc b·∫±ng b·∫£n d·ªãch n·∫øu c·∫ßn)
    C√°c b∆∞·ªõc:
       D·ªãch sang ti·∫øng Anh n·∫øu kh√¥ng ph·∫£i
       #split_and_term_extraction
       #extract_terms_only_from_sentence
       extract_opinions_only_from_clauses
       extract_category
       detect_polarity
    """

    if isinstance(sentences, str):
        sentences = [sentences]

    all_clauses = []
    total_clauses = 0
    batch_index = 11

    for i, sentence in enumerate(tqdm(sentences, desc="Translating & Extracting"), start=1):
        # --- B0: D·ªãch n·∫øu c·∫ßn ---
        sentence = translate_to_english(sentence, model_qwen, tokenizer_qwen)

        # --- B1: T√°ch clause & term ---
        # clauses = split_and_term_extraction(sentence)
        clauses = extract_terms_only_from_sentence(sentence, model_qwen, tokenizer_qwen)

        # --- B2: Tr√≠ch xu·∫•t opinion ---
        clauses = extract_opinions_only_from_clauses(clauses, model_qwen, tokenizer_qwen)

        # --- G·∫Øn ch·ªâ s·ªë & c√¢u g·ªëc (ƒë√£ d·ªãch n·∫øu c·∫ßn) ---
        for c in clauses:
            c["sentence_index"] = i
            c["sentence_original"] = sentence  # c√¢u ƒë√£ ƒë∆∞·ª£c d·ªãch sang ti·∫øng Anh


        all_clauses.extend(clauses)
        total_clauses += 1

        # --- T·ª± ƒë·ªông l∆∞u m·ªói save_every ---
        if total_clauses >= save_every:
            print(f"\nƒê√£ tr√≠ch xu·∫•t {total_clauses} clauses ‚Äî ƒëang x·ª≠ l√Ω Category & Polarity...")

            part_clauses = extract_category(all_clauses, model_cat, tokenizer_cat)
            part_clauses = detect_polarity(part_clauses)

            df_temp = pd.DataFrame(part_clauses)
            cols = [
                "sentence_index", "sentence_original",
                "clause", "term", "opinion", "category", "polarity", "polarity_score"
            ]
            df_temp = df_temp[[c for c in cols if c in df_temp.columns]]

            filename = f"{save_path}_part{batch_index}.csv"
            df_temp.to_csv(filename, index=False, encoding="utf-8-sig")
            print(f"  L∆∞u th√†nh c√¥ng: {filename}")

            all_clauses = []
            total_clauses = 0
            batch_index += 1

    # --- X·ª≠ l√Ω n·ªët ph·∫ßn c√≤n l·∫°i ---
    if all_clauses:
        print(f"\nX·ª≠ l√Ω {len(all_clauses)} clauses cu·ªëi c√πng ...")
        all_clauses = extract_category(all_clauses, model_cat, tokenizer_cat)
        all_clauses = detect_polarity(all_clauses)

        df_final = pd.DataFrame(all_clauses)
        cols = [
            "sentence_index", "sentence_original",
            "clause", "term", "opinion", "category", "polarity", "polarity_score"
        ]
        df_final = df_final[[c for c in cols if c in df_final.columns]]

        filename = f"{save_path}_final.csv"
        df_final.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"   Ho√†n th√†nh to√†n b·ªô! K·∫øt qu·∫£ l∆∞u t·∫°i: {filename}")

    return



import json

# with open('/content/test.json', 'r') as f:
#     data = json.load(f)

# all_sentences = [
#     sentence
#     for entity in data
#     for review in entity.get('reviews', [])
#     for sentence in review.get('sentences', [])
# ]

# print(all_sentences)

# print(len(all_sentences))

# all_sentences=all_sentences[:500]

"""#Pipeline"""

import pandas as pd

df_eval=pd.read_csv('/content/evaluation (6).csv')

term_counts = df_eval['term'].value_counts()

term_counts.to_csv("term_in_link.csv")

df_eval["category"].unique()

# all_sentences=list(cluster_representatives_train["sentence"])

import re

def clean_text(text):
    text = str(text).lower()                     # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng & ƒë·∫£m b·∫£o ki·ªÉu string
    text = re.sub(r"http\S+", "", text)          # B·ªè link
    text = re.sub(r"[^a-z0-9\s]", " ", text)     # B·ªè to√†n b·ªô k√Ω t·ª± ƒë·∫∑c bi·ªát (k·ªÉ c·∫£ ngo·∫∑c)
    text = re.sub(r"\s+", " ", text).strip()     # G·ªôp kho·∫£ng tr·∫Øng
    return text

df_eval

df_eval.rename(columns={"relterm":"opinion"},inplace=True)

target=df_eval[["clause","term","opinion","category","polarity"]]

test=df_eval["clause"]

all_sentences=test.tolist()

for i in range(len(all_sentences)):
    all_sentences[i] = clean_text(all_sentences[i])

len(all_sentences)

def absa_pipeline_all_sentences_no_split_save_by_batch(
    all_sentences,
    model,
    tokenizer,
    max_new_tokens_term=20,
    max_new_tokens_opinion=40,
    batch_size=100,
    verbose=False
):
    """
    ABSA pipeline WITHOUT clause splitting.
    Auto-save & download CSV every `batch_size` sentences.
    """

    from tqdm import tqdm
    import pandas as pd
    from google.colab import files

    rows_buffer = []
    file_index = 8

    iterator = tqdm(
        enumerate(all_sentences),
        total=len(all_sentences),
        desc="üîÑ ABSA Processing"
    )

    for idx, sentence in iterator:

        # ===== STEP 1: extract TERMS =====
        try:
            clauses = extract_terms_only_from_sentence_phi(
                sentence,
                model,
                tokenizer,
                max_new_tokens=max_new_tokens_term
            )
        except Exception:
            continue

        if not clauses:
            continue

        # ===== STEP 2: extract OPINIONS =====
        try:
            clauses_with_opinion = extract_opinions_only_from_clauses(
                clauses,
                model,
                tokenizer,
                max_new_tokens=max_new_tokens_opinion
            )
        except Exception:
            continue

        # ===== COLLECT ROWS =====
        for c in clauses_with_opinion:
            rows_buffer.append({
                "sentence": sentence,
                "clause": c.get("clause", ""),
                "term": c.get("term", ""),
                "opinion": c.get("opinion", "")
            })

        # ===== SAVE EVERY batch_size SENTENCES =====
        if (idx + 1) % batch_size == 0:
            df = pd.DataFrame(rows_buffer)

            output_path = f"absa_results_part_{file_index}.csv"
            df.to_csv(output_path, index=False, encoding="utf-8-sig")
            files.download(output_path)

            print(f"\n‚úÖ Saved & downloaded: {output_path} | Rows: {len(df)}")

            rows_buffer = []
            file_index += 1

    # ===== SAVE REMAINING =====
    if rows_buffer:
        df = pd.DataFrame(rows_buffer)
        output_path = f"absa_results_part_{file_index}.csv"
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        files.download(output_path)
        print(f"\n‚úÖ Saved & downloaded FINAL: {output_path} | Rows: {len(df)}")

    print("üéâ ALL DONE")

all_sentences=all_sentences[700:]

all_sentences

print(len(all_sentences))       # T·ªïng s·ªë ph·∫ßn t·ª≠
print(len(all_sentences[700:])) # S·ªë ph·∫ßn t·ª≠ t·ª´ index 700 tr·ªü ƒëi

absa_pipeline_all_sentences_no_split_save_by_batch(
    all_sentences[700:],
    model,
    tokenizer,
    max_new_tokens_term=20,
    max_new_tokens_opinion=40,
    batch_size=100,
    verbose=False
)

from google.colab import drive
drive.mount('/content/drive')

# df_results = absa_pipeline(
#     s,
#     model, tokenizer,   # d√πng cho split/normalize
#     model_cat, tokenizer_cat,     # d√πng cho category classification
#     id2label,
#     device,
# )

"""#Ph·∫ßn tr√™n l√† ph·∫ßn ch·∫°y c·∫£ pipeline"""

links=[]
for i in range(23):
  links.append(f"/content/drive/MyDrive/ABSA_results/absa_results_part{i+1}.csv")

links.append("/content/drive/MyDrive/ABSA_results/absa_results_final.csv")

import pandas as pd

dfs=[]
for url in links:
    try:
        df = pd.read_csv(url, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(url, encoding="latin1")
    print(f"Loaded: {url} ‚Üí {df.shape}")
    dfs.append(df)

# Gh√©p n·ªëi d·ªçc
data_final = pd.concat(dfs, ignore_index=True)

predict=data_final

predict.to_csv("predict.csv")

# predict=pd.read_csv("/content/test.csv")

target.rename(columns={"clause":"sentence_original"},inplace=True)

target.head(10)

predict.columns

target.columns

target["category"].unique()

predict["pred_category"].unique()

predict["category"]=predict["category"].replace("Branding","Experience")

predict.columns

truth=target

predict.rename(columns={"term":"pred_term","opinion":"pred_opinion","category":"pred_category","polarity":"pred_polarity"},inplace=True)

df_merged = pd.concat([predict,truth], axis=1)

df_merged.shape

df = df_merged.apply(lambda col: col.str.lower() if col.dtype == "object" else col)

import numpy as np

# The previous pd.concat([predict, truth], axis=1) likely created duplicate column names.
# When df[col] is called for a duplicated column name, it returns a DataFrame, not a Series,
# leading to AttributeError: 'DataFrame' object has no attribute 'str' when .str is called.

# Step 1: Make column names unique
# Create a list of unique column names, appending suffixes to duplicates.
original_cols = df.columns.tolist()
new_cols = []
seen_cols = {}

for c in original_cols:
    if c in seen_cols:
        seen_cols[c] += 1
        new_cols.append(f"{c}_{seen_cols[c]}")
    else:
        seen_cols[c] = 0
        new_cols.append(c)

df.columns = new_cols

# Step 2: Iterate through object columns and apply replacement after ensuring string type
for col in df.select_dtypes(include='object').columns:
    # Ensure the column is string type and handle NaN values gracefully before replacing.
    # Using .fillna('') makes sure all values are strings before .str.replace().
    df[col] = df[col].astype(str).str.replace('-', ' ', regex=False)

df.head(20)

"""#Evaluation

Non-discrete (term, opinion): exactMatch_f1,token_f1,rouge-l,embedding_similarity

Discrete (category, polarity): accuracy, macro F1, precision, recall ‚Üí exact match
"""

df.to_csv("evaluation_final.csv")

df.columns

# C√†i rouge-score
!pip install rouge-score

# C√†i sacrebleu
!pip install sacrebleu

import pandas as pd
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ---- Helper functions ----
def normalize(text):
    return text.lower().strip()

def exact_match_f1(pred_list, gold_list):
    tp = sum([1 for g, p in zip(gold_list, pred_list) if g == p])
    fp = sum([1 for g, p in zip(gold_list, pred_list) if g != p])
    fn = fp
    precision = tp / (tp + fp) if (tp+fp)>0 else 0.0
    recall = tp / (tp + fn) if (tp+fn)>0 else 0.0
    f1 = 2*precision*recall / (precision+recall) if (precision+recall)>0 else 0.0
    return f1

def token_f1(pred_list, gold_list):
    tp = 0
    for g, p in zip(gold_list, pred_list):
        g_tokens = set(g.split())
        p_tokens = set(p.split())
        if len(g_tokens & p_tokens) > 0:
            tp += 1
    fp = len(pred_list) - tp
    fn = len(gold_list) - tp
    precision = tp/(tp+fp) if (tp+fp)>0 else 0.0
    recall = tp/(tp+fn) if (tp+fn)>0 else 0.0
    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0
    return f1

def rouge_l(pred_list, gold_list):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    f_scores = []
    for g, p in zip(gold_list, pred_list):
        f_scores.append(scorer.score(g,p)['rougeL'].fmeasure)
    return sum(f_scores)/len(f_scores) if f_scores else 0.0

def embedding_similarity(pred_list, gold_list, model):
    sims = []
    for g, p in zip(gold_list, pred_list):
        emb_g = model.encode(g)
        emb_p = model.encode(p)
        sims.append(cosine_similarity([emb_g],[emb_p])[0][0])
    return sum(sims)/len(sims) if sims else 0.0

# ---- Initialize embedding model ----
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# ---- Non-discrete metrics (Term & Opinion) ----
non_disc_metrics = []
for key in ['term','opinion']:
    gold_list = [normalize(t) for t in df[key]]
    pred_list = [normalize(t) for t in df['pred_'+key]]

    exact_f1 = exact_match_f1(pred_list, gold_list)
    tokenf1 = token_f1(pred_list, gold_list)
    rouge = rouge_l(pred_list, gold_list)
    emb = embedding_similarity(pred_list, gold_list, embedding_model)

    non_disc_metrics.append([f"{key.capitalize()} ExactMatch_F1", exact_f1])
    non_disc_metrics.append([f"{key.capitalize()} Token_F1", tokenf1])
    non_disc_metrics.append([f"{key.capitalize()} ROUGE-L", rouge])
    non_disc_metrics.append([f"{key.capitalize()} Embedding_Similarity", emb])

non_disc_df = pd.DataFrame(non_disc_metrics, columns=["Metric","Score"])
non_disc_df["Score"] = non_disc_df["Score"].round(4)

# ---- Discrete metrics (Category & Polarity) ----
disc_metrics = []
for key in ['category','polarity']:
    y_true = df[key]
    y_pred = df['pred_'+key]

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)
    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)

    disc_metrics.append([f"{key.capitalize()} Accuracy", acc])
    disc_metrics.append([f"{key.capitalize()} Macro-F1", f1])
    disc_metrics.append([f"{key.capitalize()} Precision", prec])
    disc_metrics.append([f"{key.capitalize()} Recall", rec])

disc_df = pd.DataFrame(disc_metrics, columns=["Metric","Score"])
disc_df["Score"] = disc_df["Score"].round(4)

# ---- Display ----
print("üìå Non-discrete Metrics (Term & Opinion)\n")
print(non_disc_df.to_string(index=False))
print("\nüéØ Discrete Classification Metrics (Category & Polarity)\n")
print(disc_df.to_string(index=False))

# D√≤ng d·ª± ƒëo√°n category sai
wrong_category = df[df['category'] != df['pred_category']]

print(f"S·ªë m·∫´u category sai: {len(wrong_category)}")
wrong_category[["sentence_original","category","pred_category"]].to_csv("wrong_category.csv")  # xem 10 m·∫´u ƒë·∫ßu

wrong_category

len(wrong_category[wrong_category["category"]=="experience"])

len(df[df["category"]=="experience"])

100-35/102*100

len(wrong_category[wrong_category["category"]=="service"])

len(df[df["category"]=="service"])

100-334/1556*100

len(wrong_category[wrong_category["category"]=="amenity"])

len(df[df["category"]=="amenity"])

100-365/454*100

len(wrong_category[wrong_category["category"]=="facility"])

len(df[df["category"]=="facility"])

100-48/255*100

wrong_category[wrong_category["category"]=="amenity"].to_csv("amenity.csv")



# T·∫ßn su·∫•t t·ª´ng term
term_counts = wrong_category['term'].value_counts()

print(term_counts.head(20))  # in 20 term ph·ªï bi·∫øn nh·∫•t

wrong_category[(wrong_category["category"]=="amenity") & (wrong_category["term"].isin(["fitness center","pool","pool area"]))][["category","pred_category","clause"]].head(20)

import pandas as pd

df=pd.read_csv("/content/data (zenodo.org_records_11024326).csv")

df.columns

df["category"].unique()